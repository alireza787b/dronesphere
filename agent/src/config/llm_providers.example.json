{
  "providers": [
    {
      "name": "ollama",
      "type": "local",
      "endpoint": "http://localhost:11434",
      "models": ["llama2", "mistral", "codellama"],
      "default_model": "llama2",
      "options": {
        "temperature": 0.7,
        "top_p": 0.9
      }
    },
    {
      "name": "openai",
      "type": "api",
      "endpoint": "https://api.openai.com/v1",
      "models": ["gpt-3.5-turbo", "gpt-4", "gpt-4-turbo-preview"],
      "default_model": "gpt-3.5-turbo",
      "api_key_env": "OPENAI_API_KEY",
      "options": {
        "temperature": 0.7,
        "max_tokens": 2000
      }
    },
    {
      "name": "anthropic",
      "type": "api",
      "endpoint": "https://api.anthropic.com/v1",
      "models": ["claude-3-opus", "claude-3-sonnet", "claude-3-haiku"],
      "default_model": "claude-3-sonnet",
      "api_key_env": "ANTHROPIC_API_KEY",
      "options": {
        "temperature": 0.7,
        "max_tokens": 2000
      }
    },
    {
      "name": "deepseek",
      "type": "api",
      "endpoint": "https://api.deepseek.com/v1",
      "models": ["deepseek-chat", "deepseek-coder"],
      "default_model": "deepseek-chat",
      "api_key_env": "DEEPSEEK_API_KEY",
      "options": {
        "temperature": 0.7,
        "max_tokens": 2000,
        "top_p": 0.9
      }
    }
  ],
  "default_provider": "ollama",
  "fallback_providers": ["deepseek", "openai"],
  "retry_config": {
    "max_attempts": 3,
    "backoff_factor": 2
  }
}
